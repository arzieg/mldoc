
<!DOCTYPE html>

<html lang="de">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Deep Learning Modellanpassungen &#8212; ML Mitschriften 0.1 Dokumentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/translations.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Stichwortverzeichnis" href="genindex.html" />
    <link rel="search" title="Suche" href="search.html" />
    <link rel="prev" title="Neuronale Netzwerke" href="nn_basic.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="deep-learning-modellanpassungen">
<span id="dlanpassungen"></span><h1>Deep Learning Modellanpassungen<a class="headerlink" href="#deep-learning-modellanpassungen" title="Link zu dieser Überschrift">¶</a></h1>
<div class="section" id="bias-varianz">
<h2>Bias / Varianz<a class="headerlink" href="#bias-varianz" title="Link zu dieser Überschrift">¶</a></h2>
<p><strong>Trade off von Bias und Varianz</strong></p>
<p>„In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the
parameter estimates across samples can be reduced by increasing the bias in the estimated parameters.
The bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two
sources of error that prevent supervised learning algorithms from generalizing beyond their training set:</p>
<ul class="simple">
<li><p>The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).</p></li>
<li><p>The variance is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).</p></li>
</ul>
<p>This trade-off is universal: It has been shown that a model that is asymptotically unbiased must have unbounded variance.
The bias–variance decomposition is a way of analyzing a learning algorithm’s expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.“
[ <a class="reference external" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">Bias Variance TradeOff</a> ]</p>
<p>Beispiel:
<em>Quelle: Andrew Ng, Coursera, Improving Deep Learning</em></p>
<div class="figure align-center" id="id1">
<span id="dl-01-bias-varianz"></span><a class="reference internal image-reference" href="_images/dl_01_Bias_Varianz.png"><img alt="Bias / Varianz" src="_images/dl_01_Bias_Varianz.png" style="width: 570.0px; height: 182.0px;" /></a>
<p class="caption"><span class="caption-number">Abb. 6: </span><span class="caption-text"><a class="reference internal" href="#dl-01-bias-varianz"><span class="std std-numref">Bias Varianz (Abb. 6)</span></a></span><a class="headerlink" href="#id1" title="Link zu diesem Bild">¶</a></p>
</div>
<ul class="simple">
<li><p>links: Beispiel für ein hohes Bias. Hohe Fehlerrate bei den vorhergesagten Werten (Underfitting)</p></li>
<li><p>rechts: Beispiel für eine hohe Varianz. Das Modell auf Basis der Trainingsdaten ist zu spezifisch, so dass das Modell
schlecht an den realen Daten skalieren kann. (Overfitting)</p></li>
<li><p>Mitte: optimaler Beziehung zw. Fehlern auf der einen Seite und Komplexität des Modells auf der anderen Seite.</p></li>
</ul>
<p>Beispiel für die Beurteilung <sup>*)</sup>:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 21%" />
<col style="width: 22%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 18%" />
</colgroup>
<tbody>
<tr class="row-odd"><td></td>
<td colspan="4"><p><strong>Fehlerrate</strong></p></td>
</tr>
<tr class="row-even"><td><p>Test-Set</p></td>
<td><p>1%</p></td>
<td><p>15%</p></td>
<td><p>15%</p></td>
<td><p>0.5%</p></td>
</tr>
<tr class="row-odd"><td><p>Development-Set</p></td>
<td><p>11%</p></td>
<td><p>16%</p></td>
<td><p>30%</p></td>
<td><p>1%</p></td>
</tr>
<tr class="row-even"><td><p>Beurteilung</p></td>
<td><p>overfitting,
hohe Varianz</p></td>
<td><p>underfitting,
High Bias</p></td>
<td><p>high bias,
high variance</p></td>
<td><p>low bias,
low variance</p></td>
</tr>
</tbody>
</table>
<p><sup>*)</sup>: Alle Werte werden in Relation zu einer Fehlerrate beim Menschen beurteilt, in diesem Fall liegt dieser bei
annnährend 0% (auch Bayes Fehler genannt)</p>
<p>Zurück zu <a class="reference internal" href="#dlanpassungen"><span class="std std-ref">Deep Learning Modellanpassungen</span></a></p>
</div>
<div class="section" id="basisprozess-im-machinelearning">
<h2>Basisprozess im Machinelearning<a class="headerlink" href="#basisprozess-im-machinelearning" title="Link zu dieser Überschrift">¶</a></h2>
<div class="figure align-center" id="id2">
<span id="dl-01-basis-ml-process"></span><a class="reference internal image-reference" href="_images/dl_01_basis_ml_process.png"><img alt="Basis ML Prozess" src="_images/dl_01_basis_ml_process.png" style="width: 561.5px; height: 397.0px;" /></a>
<p class="caption"><span class="caption-number">Abb. 7: </span><span class="caption-text"><a class="reference internal" href="#dl-01-basis-ml-process"><span class="std std-numref">Basis ML Prozess (Abb. 7)</span></a></span><a class="headerlink" href="#id2" title="Link zu diesem Bild">¶</a></p>
</div>
<p>Zurück zu <a class="reference internal" href="#dlanpassungen"><span class="std std-ref">Deep Learning Modellanpassungen</span></a></p>
</div>
<div class="section" id="regularization">
<h2>Regularization<a class="headerlink" href="#regularization" title="Link zu dieser Überschrift">¶</a></h2>
<p>Über „Regularization“ kann Einfluss auf Bias und Varianz genommen werden.</p>
<div class="section" id="regularization-in-logistic-regression">
<h3>Regularization in Logistic Regression<a class="headerlink" href="#regularization-in-logistic-regression" title="Link zu dieser Überschrift">¶</a></h3>
<p>Ziel ist die Minimierung der Funktion J(w,b)</p>
<p><span class="math notranslate nohighlight">\(min(w,b) \rightarrow J(w,b)\)</span></p>
<p>Man addiert die Regularization zur Funktion. Unterschieden wird zwischen L1 und L2 Regularization, L2 wird
typischerweise verwendet.</p>
<p><span class="math notranslate nohighlight">\(L_{2}\)</span> Regularization = <span class="math notranslate nohighlight">\(\|w\|^{2}_{2}=\sum^{N_x}_{j=1}w^{2}_{j}=w^{T}w\)</span></p>
<p>Daraus folgt für die zu minimierende Funktion:
<span class="math notranslate nohighlight">\(J(w,b)=\frac{1}{m} \sum^{m}_{i=1}L(\hat y^{(i)}, y^{(i)})+ \frac{\lambda}{2m}\|w\|^{2}_{2}\)</span></p>
<p>Der Vollständigkeit halber:
<span class="math notranslate nohighlight">\(L_{1}\)</span> Regularization = <span class="math notranslate nohighlight">\(\frac{\lambda}{2m}\sum^{N_x}_{i=1}|w|=\frac{\lambda}{2m}\|w\|_{1}\)</span></p>
<p>w ist ein Vektor mit vielen Nullen, damit kann das Modell komprimiert werden (dies ist in der Praxis eher nachrangig)</p>
<p><span class="math notranslate nohighlight">\(\lambda=\)</span> ist ein Regularization Parameter und kann ebenfalls angepasst werden.</p>
</div>
<div class="section" id="regularization-in-neuronalen-netzwerken">
<h3>Regularization in Neuronalen Netzwerken<a class="headerlink" href="#regularization-in-neuronalen-netzwerken" title="Link zu dieser Überschrift">¶</a></h3>
<p>Analog dem Vorgehen aus der Regularization logistic regression ergibt sich für ein NN:</p>
<p><span class="math notranslate nohighlight">\(J(w^{[1]},b^{[1]},...,w^{[i]},b^{[i]}) = \frac{1}{m} \sum^{m}_{i=1}L(\hat y^{(i)}, y^{(i)})+
\frac{\lambda}{2m} \sum^{L}_{l=1}\|w^{[l]}\|^{2}_{F}\)</span></p>
<p>wobei <span class="math notranslate nohighlight">\(\|w^{[l]}\|^{2}=\sum^{n^{[l-1]}}_{i=1} \sum^{n^{[l]}}_{j=1}(w^{[l]}_{ij})^{2}\)</span></p>
<p><span class="math notranslate nohighlight">\(w: (n^{[l]},n^{[l-1]})\)</span> ist die Matrix mit den Dimensionen der Hidden Layer n und n-1.</p>
<p>Man spricht hier nicht von der L2 Regularization sondern von der „Frobenius norm“. Dies wird in der obigen Gleichung
durch ein runtergestelltes F dargestellt.</p>
<p>Implementierung von Gradient Descent in dieses Modell:</p>
<p>(1): <span class="math notranslate nohighlight">\(dw^{[l]} = (from \: backpropagation)+\frac{\lambda}{m}w^{[l]}\)</span></p>
<p>Für <span class="math notranslate nohighlight">\(w^{l}\)</span> gilt (2): <span class="math notranslate nohighlight">\(w^{[l]}=w{[l]}-\alpha \: dw^{[l]}\)</span></p>
<p>Setzt man (1) in (2) ergibt sich:</p>
<p><span class="math notranslate nohighlight">\(w^{[l]} = w^{[l]}-\alpha[(from \: backpropagation)+\frac{\lambda}{m}w^{[l]}]\)</span></p>
<p><span class="math notranslate nohighlight">\(w^{[l]} = w^{[l]}-\frac{\alpha \lambda}{m}w^{[l]}-\alpha(from \: backpropagation)\)</span></p>
<p>Dies kann man vereinfachen, zieht man auf der rechten Seite die Matrix w vor die Klammer, dann ergibt sich daraus,
dass von der w-Matrix jeweils der Wert <span class="math notranslate nohighlight">\((1-\frac{\alpha \lambda}{m})\)</span> abgezogen wird.</p>
</div>
<div class="section" id="dropout">
<h3>Dropout<a class="headerlink" href="#dropout" title="Link zu dieser Überschrift">¶</a></h3>
<p>Dropout ist eine andere Form der Regularization. Beim Dropout geht man durch jeden Layer und „löscht“ Knoten auf Basis
von Wahrscheinlichkeiten. Beispielsweise wird je Layer einer Wahrscheinlichkeit von 0.5 ein Knoten je Layer
eleminiert. Man löscht dann die Verbindung zu dem Knoten. Das so verkleinerte NN ist weniger Komplex und kann schneller
berechnet werden.</p>
<div class="graphviz"><img src="_images/graphviz-8cb6ba718db0677ab4d91838f97790c250b044fd.png" alt="digraph {
    rankdir=LR;
    &quot;x1&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true   ] ;
    &quot;x2&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true   ] ;
    &quot;x3&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true   ] ;
    &quot;x4&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true   ] ;
    &quot;a11&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true  ] ;
    &quot;a12&quot; [shape=circle  , regular=1,style=filled,fillcolor=red, width=.5, fixedsize=true   ] ;
    &quot;a13&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true   ] ;
    &quot;a14&quot; [shape=circle  , regular=1,style=filled,fillcolor=red, width=.5, fixedsize=true   ] ;
    &quot;a21&quot; [shape=circle  , regular=1,style=filled,fillcolor=red, width=.5, fixedsize=true  ] ;
    &quot;a22&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true   ] ;
    &quot;a23&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true   ] ;
    &quot;a24&quot; [shape=circle  , regular=1,style=filled,fillcolor=red, width=.5, fixedsize=true   ] ;
    &quot;a31&quot; [shape=circle  , regular=1,style=filled,fillcolor=red, width=.5, fixedsize=true  ] ;
    &quot;a32&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true   ] ;
    &quot;a33&quot; [shape=circle  , regular=1,style=filled,fillcolor=red, width=.5, fixedsize=true   ] ;
    &quot;a34&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true   ] ;
    &quot;a4&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true   ] ;
    &quot;y&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true   ] ;
    &quot;x1&quot; -&gt; &quot;a11&quot;;
    &quot;x1&quot; -&gt; &quot;a13&quot;;
    &quot;x2&quot; -&gt; &quot;a11&quot;;
    &quot;x2&quot; -&gt; &quot;a13&quot;;
    &quot;x3&quot; -&gt; &quot;a11&quot;;
    &quot;x3&quot; -&gt; &quot;a13&quot;;
    &quot;x4&quot; -&gt; &quot;a11&quot;;
    &quot;x4&quot; -&gt; &quot;a13&quot;;
    &quot;a11&quot; -&gt; &quot;a22&quot;;
    &quot;a11&quot; -&gt; &quot;a23&quot;;
    &quot;a13&quot; -&gt; &quot;a22&quot;;
    &quot;a13&quot; -&gt; &quot;a23&quot;;
    &quot;a22&quot; -&gt; &quot;a32&quot;;
    &quot;a22&quot; -&gt; &quot;a34&quot;;
    &quot;a23&quot; -&gt; &quot;a32&quot;;
    &quot;a23&quot; -&gt; &quot;a34&quot;;
    &quot;a32&quot; -&gt; &quot;a4&quot;;
    &quot;a34&quot; -&gt; &quot;a4&quot;;
    &quot;a4&quot; -&gt; &quot;y&quot;;
    { rank=same; &quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;, &quot;x4&quot; }
    { rank=same; &quot;a11&quot;, &quot;a12&quot;, &quot;a13&quot;, &quot;a14&quot; }
    { rank=same; &quot;a21&quot;, &quot;a22&quot;, &quot;a23&quot;, &quot;a24&quot; }
    { rank=same; &quot;a31&quot;, &quot;a32&quot;, &quot;a33&quot;, &quot;a34&quot; }
}" class="graphviz" /></div>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">ML Mitschriften</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="notation.html">Notationen</a></li>
<li class="toctree-l1"><a class="reference internal" href="mltypes.html">Machinelearning Typen</a></li>
<li class="toctree-l1"><a class="reference internal" href="supervisedlearning.html">Supervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn_basic.html">Neuronale Netzwerke</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Deep Learning Modellanpassungen</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#bias-varianz">Bias / Varianz</a></li>
<li class="toctree-l2"><a class="reference internal" href="#basisprozess-im-machinelearning">Basisprozess im Machinelearning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#regularization">Regularization</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="nn_basic.html" title="vorheriges Kapitel">Neuronale Netzwerke</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Schnellsuche</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Los" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, arzieg.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.2.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/dlanpassungen.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>