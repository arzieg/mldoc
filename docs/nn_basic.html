
<!DOCTYPE html>

<html lang="de">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neuronale Netzwerke &#8212; ML Mitschriften 0.1 Dokumentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/translations.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Stichwortverzeichnis" href="genindex.html" />
    <link rel="search" title="Suche" href="search.html" />
    <link rel="next" title="Deep Learning Modell Anpassungen" href="dlanpassungen.html" />
    <link rel="prev" title="Supervised Learning" href="supervisedlearning.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="neuronale-netzwerke">
<span id="nl-basic"></span><h1>Neuronale Netzwerke<a class="headerlink" href="#neuronale-netzwerke" title="Link zu dieser Überschrift">¶</a></h1>
<p><em>Quelle: Andrew Ng, Neural Networks and Deep Learning, Coursera, 2020</em></p>
<dl class="simple">
<dt><strong>Notation:</strong></dt><dd><p>Wichtig ist die Unterscheidung bei den hochgestellten [] - Klammern vs. den () - Klammern.
Die [] Klammern beziehen sich auf den Layer innerhalb eine NN. Die ()-Klammern beziehen sich auf
ein Element z.B. aus einem Trainingsset.</p>
</dd>
</dl>
<div class="section" id="nn-logistic-regression">
<h2>NN Logistic Regression<a class="headerlink" href="#nn-logistic-regression" title="Link zu dieser Überschrift">¶</a></h2>
<p>Andrew Ng. erarbeitet die Einführung in ein NN am Beispiel des Logistic Regression Algorithmus.</p>
<p>Bei einem NN gibt es Inputwerte (x1, x2, …), Hidden-Layer (hier nur a1) und das Ergebnis a2 (bzw. y als Extra eingezeichnet).</p>
<div class="graphviz"><img src="_images/graphviz-65fb52e58c5c3ce96e22b0d6047b71ee82828237.png" alt="digraph {
    rankdir=LR;
    &quot;x1&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true   ] ;
    &quot;x2&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true   ] ;
    &quot;x3&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true   ] ;
    &quot;a11&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true  ] ;
    &quot;a12&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true   ] ;
    &quot;a13&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true   ] ;
    &quot;a2&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true   ] ;
    &quot;y&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, width=.5, fixedsize=true   ] ;
    &quot;x1&quot; -&gt; &quot;a11&quot;;
    &quot;x1&quot; -&gt; &quot;a12&quot;;
    &quot;x1&quot; -&gt; &quot;a13&quot;;
    &quot;x2&quot; -&gt; &quot;a11&quot;;
    &quot;x2&quot; -&gt; &quot;a12&quot;;
    &quot;x2&quot; -&gt; &quot;a13&quot;;
    &quot;x3&quot; -&gt; &quot;a11&quot;;
    &quot;x3&quot; -&gt; &quot;a12&quot;;
    &quot;x3&quot; -&gt; &quot;a13&quot;;
    &quot;a11&quot; -&gt; &quot;a2&quot;;
    &quot;a12&quot; -&gt; &quot;a2&quot;;
    &quot;a13&quot; -&gt; &quot;a2&quot;;
    &quot;a2&quot; -&gt; &quot;y&quot;;
    { rank=same; &quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot; }
}" class="graphviz" /></div>
<p>In jedem Hidden Layer wird z und a (Aktvierungsfunktion) berechnet wie im Modell des Logistic Regression.
Der Outputwert geht dann, je nach Definition als Inputwert in den Hidden Layer 2 usw.</p>
<div class="graphviz"><img src="_images/graphviz-03f40e62c2477264853bbb84f0ec18818b4afadb.png" alt="digraph NN {

    node [shape=record];
    rankdir=LR;
    &quot;x&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, label=&quot;x&quot; ] ;
    &quot;W&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, label=&quot;W&quot; ] ;
    &quot;b&quot; [shape=circle  , regular=1,style=filled,fillcolor=white, label=&quot;b&quot; ] ;
    &quot;z1&quot; [shape=record  , regular=1,style=filled,fillcolor=white, width=1.75, height=0.5, fixedsize=true, label=&quot;z[1]=W[1]x+b[1]&quot;] ;
    &quot;a1&quot; [shape=record  , regular=1,style=filled,fillcolor=white, width=1.75, height=0.5, fixedsize=true,label=&quot;a[1]=sigma(z[1])&quot;] ;
    &quot;z2&quot; [shape=record  , regular=1,style=filled,fillcolor=white, width=1.75, height=0.5, fixedsize=true,label=&quot;z[2]=W[2]a[1]+b[2]&quot;   ] ;
    &quot;a2&quot; [shape=record  , regular=1,style=filled,fillcolor=white, width=1.75, height=0.5, fixedsize=true,label=&quot;a[2]=sigma(z[2])&quot;] ;
    &quot;L&quot; [shape=record  , regular=1,style=filled,fillcolor=white, width=1.75, height=0.5, fixedsize=true,label=&quot;L(a[2],y)&quot;  ] ;

    &quot;x&quot; -&gt; &quot;z1&quot;;
    &quot;W&quot; -&gt; &quot;z1&quot;;
    &quot;b&quot; -&gt; &quot;z1&quot;;
    &quot;z1&quot; -&gt; &quot;a1&quot; -&gt; &quot;z2&quot; -&gt; &quot;a2&quot; -&gt; &quot;L&quot;

    { rank=same; &quot;x&quot;, &quot;W&quot;, &quot;b&quot; }

    subgraph cluster_R1 {
      label=&quot;Hidden Layer 1&quot;
      z1 ;
      a1 ;
    }

    subgraph cluster_R2 {
      label=&quot;Output&quot;
      z2 ;
      a2 ;
    }

}" class="graphviz" /></div>
<p>oder für alle m Trainingswerte (Achtung: []=Hidden Layer, ()=Trainingsexample):</p>
<blockquote>
<div><dl>
<dt>for i = 1 to m:</dt><dd><div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(z^{[1](i)}=W^{[1]}x^{(i)}+b^{[1]}\)</span></div>
<div class="line"><span class="math notranslate nohighlight">\(a^{[1](i)}=\sigma(z^{[1](i)})\)</span></div>
<div class="line"><span class="math notranslate nohighlight">\(z^{[2](i)}=W^{[2]}a^{[1](i)}+b^{[2]}\)</span></div>
<div class="line"><span class="math notranslate nohighlight">\(a^{[2](i)}=\sigma(z^{[2](i)})\)</span></div>
</div>
</dd>
</dl>
</div></blockquote>
<dl>
<dt>bzw. als Vektorschreibweise:</dt><dd><div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(Z^{[i]}=W^{[1]}X+b^{[i]}\)</span></div>
<div class="line"><span class="math notranslate nohighlight">\(A^{[i]}=\sigma(Z^{[i]})\)</span></div>
<div class="line"><span class="math notranslate nohighlight">\(Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}\)</span></div>
<div class="line"><span class="math notranslate nohighlight">\(A^{[2]}=\sigma(Z^{[2]})\)</span></div>
</div>
</dd>
</dl>
<div class="section" id="aktivierungsfunktionen">
<h3>Aktivierungsfunktionen:<a class="headerlink" href="#aktivierungsfunktionen" title="Link zu dieser Überschrift">¶</a></h3>
<p>Bisher wurde die Sigmoid-Aktivierungsfunktion verwendet. Es macht bei NN Sinn andere Aktivierungsfunktionen zu
verwenden. Empfohlen wird alle gängigen Aktivierungsfunktion beim eigenen NN Modell auszuprobieren, welche besser
funktioniert.</p>
<p>Gängige Aktivierungsfunktion:</p>
<p><strong>Sigmoid</strong> Funktion - wird häufig nicht mehr verwendet in NN (außer im Outputlayer).</p>
<div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(g(z)=a=\frac{1}{1+e^{-z}}\)</span></div>
<div class="line"><span class="math notranslate nohighlight">\(\frac{d}{dz}g(z)=\frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}})=g(z)(1-g(z))=a-(1-a)\)</span></div>
</div>
<div class="figure align-center" id="id1">
<span id="nn-001-sigmoid-graph"></span><a class="reference internal image-reference" href="_images/nn_001_sigmoid_graph.png"><img alt="Sigmoid Function" src="_images/nn_001_sigmoid_graph.png" style="width: 1047.0px; height: 523.0px;" /></a>
<p class="caption"><span class="caption-number">Abb. 2: </span><span class="caption-text"><a class="reference internal" href="#nn-001-sigmoid-graph"><span class="std std-numref">Sigmoid Function (Abb. 2)</span></a></span><a class="headerlink" href="#id1" title="Link zu diesem Bild">¶</a></p>
</div>
<p><strong>tanh(z)</strong> - Funktion (arbeitet i.d.R. besser als die Sigmoid-Funktion, da er durch den Nullpunkt geht im Gegensatz zu
der Sigmoid-Funktion, die den Nullwert bei 0.5 hat). Für den Outputlayer ist es sinnvoll, die Sidmoid-Funktion zu
nutzen, für die Hidden-Layer die tanh-Aktivierungsfunktion.</p>
<div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(g(z)=a=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}\)</span></div>
<div class="line"><span class="math notranslate nohighlight">\(\frac{d}{dz}g(z)=1-(tanh(z))^2=1-a^2\)</span></div>
</div>
<div class="figure align-center" id="id2">
<span id="nn-002-tanh-graph"></span><a class="reference internal image-reference" href="_images/nn_002_tanh_graph.png"><img alt="tanh Function" src="_images/nn_002_tanh_graph.png" style="width: 1047.0px; height: 523.0px;" /></a>
<p class="caption"><span class="caption-number">Abb. 3: </span><span class="caption-text"><a class="reference internal" href="#nn-002-tanh-graph"><span class="std std-numref">tanh Function (Abb. 3)</span></a></span><a class="headerlink" href="#id2" title="Link zu diesem Bild">¶</a></p>
</div>
<p><strong>RelU (Rectified Linear Unit)</strong> Funktion. Die Ableitung ist 1, wenn z&gt;0 bzw. 0 wenn z&lt;0.
Vorteile: Einfache Anwendung und leichte Berechnung (im Gegensatz zu Sigmoid/tanh)
Nachteil: Beim „Lernen“ kann es vorkommen, dass der Wert bei 0 verharrt und damit kein „Lernen“ stattfindet.</p>
<div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(g(z)=a=max(0,z)\)</span></div>
<div class="line"><span class="math notranslate nohighlight">\(\begin{equation}
   \frac{d}{dz}g(z)= \begin{cases}
               0 \text{ wenn z &lt; 0 } \\
               1 \text{ wenn z &gt; 0 } \\
               undef \text{ wenn z = 0 }
            \end{cases}
\end{equation}\)</span></div>
</div>
<div class="figure align-center" id="id3">
<span id="nn-003-relu-graph"></span><a class="reference internal image-reference" href="_images/nn_003_relu_graph.png"><img alt="RelU Function" src="_images/nn_003_relu_graph.png" style="width: 1047.0px; height: 523.0px;" /></a>
<p class="caption"><span class="caption-number">Abb. 4: </span><span class="caption-text"><a class="reference internal" href="#nn-003-relu-graph"><span class="std std-numref">RelU Function (Abb. 4)</span></a></span><a class="headerlink" href="#id3" title="Link zu diesem Bild">¶</a></p>
</div>
<p><strong>leaky RelU</strong> - wenn z&lt;0, dann ist die Ableitung negativ und nicht 0 wie bei RelU. Dies hilft beim
Gradient Descent Verfahren (hebt den Nachteil von RelU auf).</p>
<div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(a=max(0.01z,z)\)</span></div>
<div class="line"><span class="math notranslate nohighlight">\(\begin{equation}
   \frac{d}{dz}g(z)= \begin{cases}
               0.01 \text{ wenn z &lt; 0 } \\
               1 \text{ wenn z } \geq \; 0
            \end{cases}
\end{equation}\)</span></div>
</div>
<div class="figure align-center" id="id4">
<span id="nn-004-lrelu-graph"></span><a class="reference internal image-reference" href="_images/nn_004_lrelu_graph.png"><img alt="Leaky RelU Function" src="_images/nn_004_lrelu_graph.png" style="width: 1047.0px; height: 523.0px;" /></a>
<p class="caption"><span class="caption-number">Abb. 5: </span><span class="caption-text"><a class="reference internal" href="#nn-004-lrelu-graph"><span class="std std-numref">Leaky RelU Function (Abb. 5)</span></a></span><a class="headerlink" href="#id4" title="Link zu diesem Bild">¶</a></p>
</div>
</div>
<div class="section" id="forward-propagation">
<h3>Forward Propagation<a class="headerlink" href="#forward-propagation" title="Link zu dieser Überschrift">¶</a></h3>
<p>Analog zu dem Logistic Regression Algorithmus berechnet sich die Forward Propagation für ein NN
wie folgt:</p>
<blockquote>
<div><div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}\)</span></div>
<div class="line"><span class="math notranslate nohighlight">\(A^{[l]}=g^{[l]}(Z^{[l]})=\sigma(Z^{[l]})\)</span></div>
</div>
</div></blockquote>
<p>Man beachte: X im Layer 0 kann auch als <span class="math notranslate nohighlight">\(A^{[0]}\)</span> bezeichnet werden.</p>
</div>
<div class="section" id="backward-propagation">
<h3>Backward Propagation<a class="headerlink" href="#backward-propagation" title="Link zu dieser Überschrift">¶</a></h3>
<blockquote>
<div><div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(dZ^{[l]}=dA^{[l]}*g^{[l]'}(Z^{[l]})\)</span></div>
<div class="line"><span class="math notranslate nohighlight">\(dW^{[l]}=\frac{1}{m}dZ^{[l]}A^{[l-1]T}\)</span></div>
<div class="line"><span class="math notranslate nohighlight">\(db^{[l]}=\frac{1}{m}np.sum(dZ^{[l]}, axis=1, keepdims=true)\)</span></div>
<div class="line"><span class="math notranslate nohighlight">\(dA^{[l-1]}=W^{[l]T}dZ^{[l]}\)</span></div>
</div>
</div></blockquote>
</div>
<div class="section" id="initialisierung-des-nn">
<h3>Initialisierung des NN<a class="headerlink" href="#initialisierung-des-nn" title="Link zu dieser Überschrift">¶</a></h3>
<p>Bei der Initialisierung des NN ist die Matrix W und der Vektor b mit Werten vorzubelegen. Man kann zeigen, dass
b ein Null-Vektor sein kann, W sollte aber mit Zufallszahlen initialisiert werden. Wenn W ebenfalls eine Null-Matrix
ist, würde bei der Backpropagation symmetrische Werte errechnet werden zw. den Hidden-Layern (so dass man eigentlich
das Modell auf ein Hidden Layer reduzieren kann.)
Bei der Initialisierung ist weiterhin darauf zu achten, dass die W-Matrix mit kleinen Werten initialisiert wird.
Dies liegt daran, dass die Aktivierungsfunktion bei &gt; 1 oder &lt; 1 bereits auf einen Wert limitiert wird (0 oder 1).
Um hier eine hohe Variabilität zu haben, sollten die Werte für W -1 &lt; w &lt; 1 sein.</p>
<div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(w^{[1]}=np.random.rand(2,2)*0.01\)</span> (anstelle von 0.01 kann auch ein anderer kleiner Wert genommen werden).</div>
<div class="line"><span class="math notranslate nohighlight">\(w^{[2]}=np.random.rand(1,2)*0.01\)</span> (anstelle von 0.01 kann auch ein anderer kleiner Wert genommen werden).</div>
</div>
</div>
<div class="section" id="hyperparameter">
<h3>Hyperparameter<a class="headerlink" href="#hyperparameter" title="Link zu dieser Überschrift">¶</a></h3>
<p>Ein NN wird nicht nur durch die Funktionen beeinflusst sondern auch von anderen Parametern, z.B.</p>
<ul class="simple">
<li><p>Learning Rate Alpha</p></li>
<li><p>Anzahl Iterationen</p></li>
<li><p>Anzahl Hidden Layer N</p></li>
<li><p>Anzahl Hidden Units</p></li>
<li><p>Wahl der Aktivierungsfunktionen</p></li>
</ul>
<p>Diese Werte werdem in einem NN definiert und werden als Hyperparameter bezeichnet. Sie kontrollieren die
Berechnung von W und b.</p>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">ML Mitschriften</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="notation.html">Notationen</a></li>
<li class="toctree-l1"><a class="reference internal" href="mltypes.html">Machinelearning Typen</a></li>
<li class="toctree-l1"><a class="reference internal" href="supervisedlearning.html">Supervised Learning</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Neuronale Netzwerke</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#nn-logistic-regression">NN Logistic Regression</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="dlanpassungen.html">Deep Learning Modellanpassungen</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="supervisedlearning.html" title="vorheriges Kapitel">Supervised Learning</a></li>
      <li>Next: <a href="dlanpassungen.html" title="nächstes Kapitel">Deep Learning Modell Anpassungen</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Schnellsuche</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Los" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, arzieg.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.2.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/nn_basic.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>